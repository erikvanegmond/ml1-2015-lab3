{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Gaussian process regression\n",
    "\n",
    "### Machine Learning 1, September 2015\n",
    "\n",
    "* The lab exercises should be made in groups of two, three or four people.\n",
    "* The deadline is October 25th (Sunday) 23:59.\n",
    "* Assignment should be sent to Philip Versteeg (p.j.j.p.versteeg@uva.nl). The subject line of your email should be \"lab\\#\\_lastname1\\_lastname2\\_lastname3\".\n",
    "* Put your and your teammates' names in the body of the email.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"lab01\\_Kingma\\_Hu\", the attached file should be \"lab01\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* NOTE: Make sure we can run your notebook / scripts!\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bxp}{\\mathbf{x}^{'}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\ba}{\\mathbf{a}}$\n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$\n",
    "$\\newcommand{\\bq}{\\mathbf{q}}$\n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$\n",
    "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bT}{\\mathbf{T}}$\n",
    "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\xm}{\\mathbf{x}_m}$\n",
    "$\\newcommand{\\xn}{\\mathbf{x}_n}$\n",
    "$\\newcommand{\\y}{\\mathbf{y}}$\n",
    "$\\newcommand{\\K}{\\mathbf{K}}$\n",
    "$\\newcommand{\\zero}{\\mathbf{0}}$\n",
    "$\\newcommand{\\yi}{\\y_i}$\n",
    "$\\newcommand{\\thetav}{\\mathbf{\\theta}}$\n",
    "$\\newcommand{\\t}{\\mathbf{t}}$\n",
    "$\\newcommand{\\x}{\\mathbf{x}}$\n",
    "$\\newcommand{\\tN}{\\mathbf{t}_N}$\n",
    "$\\newcommand{\\xN}{\\mathbf{x}_N}$\n",
    "$\\newcommand{\\k}{\\mathbf{k}}$\n",
    "$\\newcommand{\\C}{\\mathbf{C}}$\n",
    "$\\newcommand{\\CN}{\\mathbf{C}_N}$\n",
    "$\\newcommand{\\KN}{\\mathbf{K}_N}$\n",
    "$\\newcommand{\\eyeN}{\\mathbf{I}_N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian process regression\n",
    "\n",
    "For this Lab we will be refer to Bishop sections 6.4.2 and 6.4.3. You may also want to refer to Rasmussen's Gaussian Process text which is available online at http://www.gaussianprocess.org/gpml/chapters/ and especially to the project found at http://www.automaticstatistician.com/index.php by Ghahramani for some intuition in GP.  To understand Gaussian processes, it is highly recommended understand how marginal, partitioned Gaussian distributions can be converted into conditional Gaussian distributions.  This is covered in Bishop 2.3 and summarized in Eqns 2.94-2.98.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinusoidal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same data generating function that we used previously for regression.  You can change sigma/beta, but keep it reasonable.  Definitely play around once you have things working.  Make use of these functions as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pylab as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "beta  = 1.0 / pow(sigma,2) # this is the beta used in Bishop Eqn. 6.59\n",
    "N_test = 100\n",
    "x_test = np.linspace(-1,1,N_test); \n",
    "mu_test = np.zeros( N_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def true_mean_function( x ):\n",
    "    return np.sin( 2*pi*(x+1) )\n",
    "\n",
    "def add_noise( y, sigma ):\n",
    "    return y + sigma*np.random.randn(len(y))\n",
    "\n",
    "def generate_t( x, sigma ):\n",
    "    return add_noise( true_mean_function( x), sigma )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f8f1bdc90>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXHsGtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4f8f1bdbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test = true_mean_function( x_test )\n",
    "t_test = add_noise( y_test, sigma )\n",
    "pp.plot( x_test, y_test, 'b-', lw=2)\n",
    "pp.plot( x_test, t_test, 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sampling from the Gaussian process prior (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement Gaussian process regression using the kernel function in Bishop Eqn. 6.63.  \n",
    "\n",
    "#### 1.1 k_n_m( xn, xm, thetas ) (10 points)\n",
    "To start, implement function \"k_n_m( xn, xm, thetas )\" that takes scalars $\\xn$ and $\\xm$, and a vector of $4$ thetas, and computes the kernel function Bishop Eqn. 6.63 (10 points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_n_m(xn, xm, thetas):\n",
    "    return thetas[0]*np.exp(-thetas[1]*0.5*np.linalg.norm(xn-xm)**2)+thetas[2]+thetas[3]*np.dot(xn.T, xm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 computeK( X1, X2, thetas ) (5 points)\n",
    "Eqn 6.60 is the marginal distribution of mean ouput of $N$ data vectors: $p(\\y) = \\mathcal{N}(\\zero, \\K)$.  Notice that the expected mean function is $0$ at all locations, and that the covariance is a $N$ by $N$ kernel matrix $\\K$.  Write a function \"computeK( X1, X2, thetas )\" that computes the kernel matrix. Hint: use k_n_m as part of an innner loop (of course, there are more efficient ways of computing the kernel function making better use of vectorization, but that is not necessary) (5 points).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeK(X1, X2, thetas):\n",
    "    K = np.zeros((len(X1), len(X2)) )\n",
    "    for n in range(len(X1)):\n",
    "        for m in range(len(X2)):\n",
    "            K[n][m] = k_n_m(X1[n], X2[m], thetas)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Plot function samples (15 points)\n",
    "Now sample mean functions at the x_test locations for the theta values in Bishop Figure 6.5, make a figure with a 2 by 3 subplot and make sure the title reflects the theta values (make sure everything is legible).  In other words, sample $\\yi \\sim \\mathcal{N}(\\zero, \\K_{\\thetav})$.  Make use of numpy.random.multivariate_normal().  On your plots include the expected value of $\\y$ with a dashed line and fill_between 2 standard deviations of the uncertainty due to $\\K$ (the diagonal of $\\K$ is the variance of the model uncertainty) (15 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.42888865 -1.43876624 -1.44355886 -1.44319819 -1.43764648 -1.42689644\n",
      " -1.41097104 -1.38992238 -1.36383054 -1.33280283 -1.29697051 -1.25648793\n",
      " -1.21152955 -1.16228783 -1.10897027 -1.05179794 -0.9910021  -0.92682283\n",
      " -0.85950637 -0.7893036  -0.71646865 -0.64125717 -0.56392551 -0.48472996\n",
      " -0.40392594 -0.32176838 -0.23851041 -0.15440445 -0.06970179  0.01534714\n",
      "  0.10049286  0.18548637  0.27007933  0.35402409  0.4370739   0.51898396\n",
      "  0.59951128  0.67841605  0.75546298  0.83042212  0.90307089  0.97319586\n",
      "  1.04059394  1.10507459  1.16646276  1.22459844  1.27934088  1.33056882\n",
      "  1.37818186  1.42210224  1.46227472  1.4986681   1.53127387  1.56010686\n",
      "  1.58520384  1.60662239  1.62443915  1.63874807  1.64965716  1.65728711\n",
      "  1.66176747  1.66323371  1.66182458  1.65767862  1.65093137  1.64171293\n",
      "  1.63014475  1.61633768  1.60039067  1.58238895  1.56240307  1.5404889\n",
      "  1.51668778  1.49102729  1.46352225  1.43417701  1.40298733  1.36994337\n",
      "  1.33503213  1.29824172  1.25956363  1.21899736  1.17655349  1.13225681\n",
      "  1.08614996  1.0382968   0.98878402  0.93772385  0.88525596  0.83154736\n",
      "  0.77679364  0.72121859  0.66507287  0.60863203  0.55219511  0.49608051\n",
      "  0.44062312  0.3861699   0.33307524  0.28169595]\n",
      "[-5.83229777 -5.86699381 -5.90113597 -5.93476154 -5.96781704 -6.00015949\n",
      " -6.03155402 -6.06167718 -6.09012149 -6.11639984 -6.13995622 -6.16017154\n",
      " -6.17637794 -6.1878677  -6.19390601 -6.19374548 -6.18663737 -6.17184401\n",
      " -6.14865483 -6.11639314 -6.07443185 -6.02220078 -5.95919768 -5.88499331\n",
      " -5.79924114 -5.7016802  -5.59213908 -5.47053864 -5.33689277 -5.19130795\n",
      " -5.03398086 -4.86519658 -4.68532497 -4.49481513 -4.29418991 -4.08404106\n",
      " -3.86502148 -3.63783852 -3.40324651 -3.1620399  -2.91504552 -2.66311384\n",
      " -2.40711274 -2.14792108 -1.88641782 -1.62348051 -1.35997444 -1.09674858\n",
      " -0.83463026 -0.57441847 -0.31688091 -0.06274862  0.18728621  0.43257499\n",
      "  0.67251316  0.90654314  1.13415494  1.3548863   1.56832343  1.77409913\n",
      "  1.97189334  2.16142868  2.34247114  2.5148259   2.67833533  2.8328752\n",
      "  2.97835237  3.11470056  3.2418795   3.35986922  3.46867031  3.56829892\n",
      "  3.65878719  3.74018029  3.81253724  3.8759284   3.93043804  3.97616358\n",
      "  4.0132185   4.04173222  4.06185478  4.073759    4.07764218  4.07373123\n",
      "  4.06228589  4.04360117  4.01801111  3.98589211  3.94766655  3.90380166\n",
      "  3.85481517  3.80127408  3.74379353  3.68303964  3.61972439  3.55460522\n",
      "  3.48848077  3.4221861   3.35658769  3.29257789]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:4: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    }
   ],
   "source": [
    "thetas_list = [[1, 4, 0, 0], [9, 4, 0, 0]]\n",
    "for thetas in thetas_list:\n",
    "    K = computeK(x_test, x_test, thetas)\n",
    "    print numpy.random.multivariate_normal(np.zeros(K.shape[0]), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predictive distribution (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have sampled mean functions from the prior.  We can draw actual data $\\t$ two ways.  The first way is generatively, by first sampling $\\y | \\K$, then sampling $\\t | \\y, \\beta$ (Eqns 6.60 followed by 6.59).  The second way is to integrate over $\\y$ (the mean draw) and directly sample $\\t | \\K, \\beta$ using Eqn 6.61.    This is the generative process for $\\t$.  Note that we have not specified a distribution over inputs $\\x$;  this is because Gaussian processes are conditional models.  Because of this we are free to generate locations $\\x$ when playing around with the GP; obviously a dataset will give us input-output pairs.\n",
    "\n",
    "Once we have data, we are interested in the predictive distribution (note: the prior is the predictive distribution when there is no data).  Consider the joint distribution for $N+1$ targets, given by Eqn 6.64.  Its covariance matrix is composed of block components $\\CN$, $\\k$, and $c$.  The covariance matrix $CN$ for $\\tN$ is $\\CN = \\KN + \\eyeN / \\beta$.  We have just made explicit the size $N$ of the matrix; $N$ is the number of training points.  The kernel vector $\\k$ is a $N$ by $1$ vector of kernel function evaluations between the training input data and the test input vector.  The scalar $c$ is a kernel evaluation at the test input.\n",
    "\n",
    "#### 2.1 gp_predictive_distribution(...) (10 points)\n",
    "Write a function \"gp_predictive_distribution(x_train, t_train, x_test, theta, beta, C = None)\" that computes  Eqns 6.66 and 6.67, except allow for an arbitrary number of test points (not just one) and now the kernel matrix is for training data.  By having C as an optional parameter, we can avoid computing it more than once (for this problem it is unimportant, but for real problems this is an issue).  The function should compute $\\C$, $\\k$, and $c$ and the mean and noise functions.  Do not forget: the computeK function computes $\\K$, not $\\C$! (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 gp_log_likelihood(...) (10 points)\n",
    "Later, to learn the hyperparameters, we will need to compute the log-likelihood of the of the training data.  Implicitly, this is conditioned on the value setting for $\\thetav$.  Write a function \"gp_log_likelihood( x_train, t_train, theta, C = None, invC = None )\", where C and invC can be stored and reused.  (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Plotting (10 points)\n",
    "Repeat the 6 plots above, but this time conditioned on the training points.  Use the sinuosoidal data generator to create 2 training points where x is sampled uniformly between $-1$ and $1$.  For these plots, feel free to use the provided function \"gp_plot\".  Make sure you put the parameters in the title and this time also the log-likelihood. (10 points)  Try to understand the two types of uncertainty!  If you do not use \"gp_plot\", please add a fill between for the model and target noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gp_plot( x_test, y_test, mu_test, var_test, x_train, t_train, theta, beta ):\n",
    "    # x_test:   the test data\n",
    "    # y_test:   the true function at x_test\n",
    "    # mu_test:  predictive mean at x_test\n",
    "    # var_test: predictive covariance at x_test \n",
    "    # t_train:  the training values\n",
    "    # theta:    the kernel parameters\n",
    "    # beta:     the precision (known)\n",
    "    \n",
    "    # the reason for the manipulation is to allow plots separating model and data stddevs.\n",
    "    std_total = np.sqrt(np.diag(var_test))         # includes all uncertainty, model and target noise \n",
    "    std_model = np.sqrt( std_total**2 - 1.0/beta ) # remove data noise to get model uncertainty in stddev\n",
    "    std_combo = std_model + np.sqrt( 1.0/beta )    # add stddev (note: not the same as full)\n",
    "    \n",
    "    pp.plot( x_test, y_test, 'b', lw=3)\n",
    "    pp.plot( x_test, mu_test, 'k--', lw=2 )\n",
    "    pp.fill_between( x_test, mu_test+2*std_combo,mu_test-2*std_combo, color='k', alpha=0.25 )\n",
    "    pp.fill_between( x_test, mu_test+2*std_model,mu_test-2*std_model, color='r', alpha=0.25 )\n",
    "    pp.plot( x_train, t_train, 'ro', ms=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 More ploting (5 points)\n",
    "Repeat the 6 plots above, but this time conditioned a new set of 10 training points. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Learning the hyperparameters (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the values of the parameter $\\thetav$ can be very tricky for Gaussian processes in general, but when the data is univariate like ours, we can visualize the fit and see how plausible it looks.\n",
    "\n",
    "#### 3.1 Derivatives (5 points)\n",
    "Maximum likelihood or MAP learning is the most common way of setting the parameters, though a fully Bayesian approach is possible too.  We will look at ML today.  For this, we start with the dervivative of the log-likelihood with respect to the parameters $\\thetav$; this is Eqn 6.70.  This, in turn, requires the derivative of the kernel matrix $\\CN$ wrt $\\thetav$.  This is the matrix of element-wise derivatives of the kernel function.  Write the derivatives for $\\theta_0$ to $\\theta_3$ for our kernel function (5 points).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[___answer here___]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Questions (5 points)\n",
    "Which parameters in $\\thetav$ are unconstrained, that is, where any positive/ negative values are valid? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[___answer here___]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 More derivatives (5 points)\n",
    "For parameters that are constrained to be positive, the usual approach is to use the exponential of the free-parameter in the kernel function, but perform gradient ascent on the unconstrained values.  Consider the case  $\\theta_i = \\exp( \\phi_i)$, where $\\phi_i$ is unconstrained.  Write the derivative for $\\phi_i$ in terms of the derivatives you already computed (5 points).  Hint: use the chain rule and do not repeat the full derivation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[___answer here___]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Grid search (10 points)\n",
    "Grid-search: for the same training set you have above, perform a small grid search over $\\thetav$ (try at least 20 combinations).  Have your grid-search loop or function print out rows of log-likelihood + $\\thetav$ sorted by best to worst.  Use the log-likelihood to select the best $\\thetav$ and the worst.  Plots both the same way as the subplots above (ie a 1 by 2 subplot of best and worst). (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[___answer here___]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Questions (10 points)\n",
    "Selecting kernel functions can be somewhat of an art.  There are charateristics of kernel functions that are useful for some data sets, but not others.  Complicating the matter is the ability to combine kernels with different characteristics (long term trends + seasonal fluctuations).  Describe the charactistics of the kernel function we are using in terms of (signal, scale, offsets, etc). You may want to play around with $\\thetav$ and see what each parameter does/affects/etc.  (5 points)  Describe why the best parameters work well for the training data and explain why the bad parameter settings perform poorly (in terms of the first part of the question).  (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[___answer here___]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Bonus: Implementation (20 points)\n",
    "Implement gradient-ascent (or descent if you wish) using the combination of a) the log-likelihood objective function and b) the gradients you calculated above.  Run on the training data above and show the log-likehood curve as it learns and a plot of the final model.  Feel free to use available software (eg search for \"minimize.py\" which uses conjugate gradient descent, or something in scipy).  NB: log-likelihood should be monotonically increasing.  You are encouraged to also search and use \"checkgrad\".  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
